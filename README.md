# CarDrive_1

tensorflow를 활용하여 차량이 스스로 움직일 수 있게 학습시키는 강화학습 프로젝트
- tensorflow를 활용해 DQN을 구성하고 이를 강화학습에 활용한다. 구현에는 python을 사용했다.
- 그래픽을 통해 학습 상황을 포시한다. 이의 구현에는 C#을 사용했다.
- DQN의 입력값으로 자신의 속도와 차량 정면의 5가지 각도에서 측정한 벽과의 거리를 입력한다.
- 이에 대한 출력으로는 키보드의 화살표 키로 표현 가능한 9가지 경우의 수에 대한 점수값을 출력받는다.
- 차량은 타원형의 트랙을 따라 움직이며, 행동의 결과에 따라 일정한 보상을 받는다.
- 입력값에 따른 행동과 점수의 관계를 이용해 차량이 최적의 경로로 주행하도록 학습시키는 것이 프로젝트의 목표이다.

2인 팀프로젝트이지만 거의 대부분의 개발을 맡았다.


작동 화면
==================
작동 화면들은 현재 코드로 볼 수 있는 화면이 아닌, 프로젝트의 진행 상황에 따라 다른 여러 시점의 화면이다.

![image1](./sample_images/1.png)

- 트랙은 화면과 같이 타원형으로 구성된다.
- 차량은 아래쪽 중앙에서 출발하여 트랙을 따라 시계 반대 방향으로 주행한다.
- 트랙을 벗어나면 트랙 이탈로 보상을 잃고 처음부터 다시 출발한다.
- 트랙에 일정 거리마다 있는 선은 체크 포인트로, 이를 통과한 차량은 보상을 얻는다.

![image2](./sample_images/5.png)

- 차량이 다음으로 통과해야 하는 체크 포인트는 색으로 표시된다.
- 아래쪽 중앙의 출발선을 통과하면 추가보상을 얻는다.
- 차량으로부터 뻗어있는 5개의 선은 시야선으로, 해당 차량의 시야를 나타낸다. 
- 시야선보다 더 넓은 시야각이나 더 멀리 떨어진 벽은 차량이 인식할 수 없다.


개발 동기 등
==================
tensorflow의 활용법을 혼자 공부하면서 이를 활용한 프로젝트를 하나 진행하고 싶었다. 
이에 강화학습을 통해 학습시키는 모델을 만들어보고자 하였고, 이전까지 활용했던 C#의 코드들을 활용해 그 결과를 직접 보고자 했다.

단순히 학습을 시키는 것 뿐만 아니라 학습 과정과 학습 단계에 따른 결과값의 변화, 학습의 결과를 눈으로 보는 것이 이 프로젝트의 목표이다.
- 따라서 차량이 움직이는 것을 직접 볼 수 있게 그래픽 요소를 추가하고, 학습을 빠르게 시키는 것보다 차량의 이동도 볼 수 있도록 계산 속도를 타이머로 조절했다.

- 학습에 성공하여 트랙을 계속 돌고있는 차들을 보며 더 빠르게 학습시키기 위해 파라미터들을 조정하다가 오히려 더 망가지는 경우가 많았다...
- 노트북으로 학습을 시켰는데, 이 과정에서 발열이 너무 심해 오랫동안 학습을 시키기는 힘들었다.

문제, 해결, 구현
==================
이전의 C# 코드에서 발전시켜온 공통적으로 사용할 코드들은 따로 개별 프로젝트로 분리하고, 이 프로젝트의 구현에만 집중하였다.

기본 작동
---------------

구현

차량은 입력에 따라 일정한 속도를 얻는 것이 아니라 가속도를 얻는다.
- 따라서 경로 선택을 일정하게 해야 실질적으로 선택된 경로로 진행하게 된다.
- 이로 인해 초창기에는 아주 잠깐씩의 전진을 통해 매우 안전하게 주행하려는 모습을 보였다.
이를 해결하기 위해 시간마다 점수를 조금씩 깎는 식으로 패널티를 부여하였다.
결과적으로 최대한 빠르게 가려는 성질이 추가되었다.


원래는 출발선만 있고 이를 통과해야만 점수를 부여했는데, 이러자 보상을 얻지 못해 아무것도 하지 않으려고 하기에 체크포인트를 추가했다.
대신 출발선의 보상을 강화했다.

하지만 이에 따른 부작용도 많었다.
가만히 있는 것보다는 움직이는 것이 더 많은 보상을 얻으니 일단 차량이 움직이기는 하는데
시간에 따른 패널티를 덜 받기위해 아예 시작하자마자 벽에 박아버려서 점수 계산을 끝내는 꼼수를 부리기 시작했다.
그렇다고 트랙 이탈의 패널티를 증가시카자, 이번에는 아예 움직이지를 않는다.


초반 몇번의 시도에서 첫번째 체크포인트를 성공적으로 통과하는 것이 학습의 성공률과 속도에 매우 큰 영향을 주었다. 
- 초반에 실패만을 반복한다면 보상을 받지 못해 모든 경로가 위험하다고 판단하게 된다. 따라서 후진을 하거나 움직이지 않으려고 한다.
- 이를 해결하기 위해 초반 몇번의 시도에서만 적용되는 여러가지 변형을 통해 더 빠르게 진행할 수 있도록 구현했다.

- 초반에는 경로 선택의 랜덤성을 늘려 안전하지 않다고 판단된 경로로도 다시 시도할 가능성을 높였다.
- 초반에는 경로 선택의 폭을 줄였다. 총 9가지 경로 중, 첫 몇번은 전진의 3가지 경로만, 다음 몇번은 후진하지 않는 6가지 경로만 선택하게 된다.


학습이 너무 느리게 이루어지자 여러가지 방법을 통해 해결하려고 했다.
- 신경망의 크기를 조절해가며 적정선을 찾아내고자 했다.
- 학습 초반에 보상을 받을 확률을 높일 방법을 적용하여 최적의 경로를 더 빠르게 찾아낼 수 있도록 유도했다.
- 계산을 수행할 차량을 한번에 5개씩 수행하도록 수정했다.
- 계산량이 증가하자 tensorflow-gpu 로 교체하여 계산을 수행했다.

때로는 입력할 정보를 제한하는 것이 더 나은 경우가 많았다.
원래는 시야선이 무제한이였지만 현실성을 위해서도, 성능을 위해서도 시야의 범위를 제한했다.
마찬가지로 원래는 더 많은 정보를 dqn에 입력했지만, 필요없는 정보는 전달하지 않는 것이 더 좋은 성능을 보였다.

보상으로 제공하는 점수값의 조정이 매우 힘들었다.

